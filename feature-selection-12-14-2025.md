# ğŸ“š Feature Selection Methodology - Complete Technical Documentation
## Parkinson's Disease Detection from Voice Signals

---

# ğŸ“‘ TABLE OF CONTENTS

1. [Why Feature Selection?](#1-why-feature-selection)
2. [Our Approach Overview](#2-our-approach-overview)
3. [Filter Methods (4 Methods)](#3-filter-methods)
   - 3.1 Mutual Information
   - 3.2 Chi-Squared (Ï‡Â²)
   - 3.3 ANOVA F-Statistic
   - 3.4 Correlation-Based Feature Selection (CFS)
4. [Wrapper Methods (2 Methods)](#4-wrapper-methods)
   - 4.1 RFE with Logistic Regression
   - 4.2 RFE with Random Forest
5. [Embedded Methods (3 Methods)](#5-embedded-methods)
   - 5.1 Lasso (L1 Regularization)
   - 5.2 Random Forest Feature Importance
   - 5.3 Gradient Boosting Feature Importance
6. [Consensus Strategy](#6-consensus-strategy)
7. [Final Results & Analysis](#7-final-results--analysis)

---

# 1. WHY FEATURE SELECTION?

## 1.1 à¦†à¦®à¦¾à¦¦à§‡à¦° Problem Statement

à¦†à¦®à¦¾à¦¦à§‡à¦° à¦•à¦¾à¦›à§‡ **51à¦Ÿà¦¿ features** à¦†à¦›à§‡ 398à¦Ÿà¦¿ audio samples à¦¥à§‡à¦•à§‡ extract à¦•à¦°à¦¾à¥¤ à¦•à¦¿à¦¨à§à¦¤à§ à¦¸à¦¬ features à¦•à¦¿ à¦¸à¦®à¦¾à¦¨ important? à¦¨à¦¾!

### à¦¸à¦®à¦¸à§à¦¯à¦¾à¦—à§à¦²à§‹:

1. **Curse of Dimensionality (à¦®à¦¾à¦¤à§à¦°à¦¾à¦° à¦…à¦­à¦¿à¦¶à¦¾à¦ª)**
   ```
   à¦†à¦®à¦¾à¦¦à§‡à¦° data: 398 samples Ã— 51 features
   
   Rule of thumb: n_samples >= 10 Ã— n_features
   à¦†à¦®à¦¾à¦¦à§‡à¦° à¦¦à¦°à¦•à¦¾à¦°: 51 Ã— 10 = 510 samples
   à¦†à¦®à¦¾à¦¦à§‡à¦° à¦†à¦›à§‡: 398 samples âŒ
   
   à¦¤à¦¾à¦‡ feature à¦•à¦®à¦¾à¦¤à§‡ à¦¹à¦¬à§‡!
   ```

2. **Overfitting Problem**
   - à¦¬à§‡à¦¶à¦¿ features = model noise à¦¶à§‡à¦–à§‡
   - Training accuracy à¦¬à¦¾à¦¡à¦¼à§‡, test accuracy à¦•à¦®à§‡
   - Generalization à¦–à¦¾à¦°à¦¾à¦ª à¦¹à¦¯à¦¼

3. **Redundancy (Repeated Information)**
   - à¦•à¦¿à¦›à§ features à¦à¦•à¦‡ information à¦¦à§‡à¦¯à¦¼
   - Example: MFCC_1_mean à¦†à¦° MFCC_1_std correlated à¦¹à¦¤à§‡ à¦ªà¦¾à¦°à§‡
   - à¦¦à§à¦‡à¦Ÿà¦¾ à¦°à¦¾à¦–à¦²à§‡ computation à¦¬à¦¾à¦¡à¦¼à§‡, benefit à¦¬à¦¾à¦¡à¦¼à§‡ à¦¨à¦¾

4. **Noise (à¦¶à§à¦§à§ confusion)**
   - à¦•à¦¿à¦›à§ features à¦¤à§‡ PD vs HC à¦¤à§‡ à¦•à§‹à¦¨à§‹ difference à¦¨à§‡à¦‡
   - à¦à¦—à§à¦²à§‹ model à¦•à§‡ confuse à¦•à¦°à§‡

## 1.2 à¦†à¦®à¦¾à¦¦à§‡à¦° Goal

```
Original: 51 features
Target: ~25 features (50% reduction)
Strategy: Multiple methods â†’ Consensus
```

---

# 2. OUR APPROACH OVERVIEW

## 2.1 à¦¤à¦¿à¦¨ à¦§à¦°à¦¨à§‡à¦° Feature Selection Methods

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FEATURE SELECTION                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  FILTER         â”‚  WRAPPER        â”‚  EMBEDDED               â”‚
â”‚  (Model-free)   â”‚  (Model-based)  â”‚  (Training-integrated)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Fast          â”‚ â€¢ Accurate      â”‚ â€¢ Automatic             â”‚
â”‚ â€¢ Simple        â”‚ â€¢ Slow          â”‚ â€¢ Model-specific        â”‚
â”‚ â€¢ Independent   â”‚ â€¢ Iterative     â”‚ â€¢ Efficient             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2.2 à¦†à¦®à¦¾à¦¦à§‡à¦° 9à¦Ÿà¦¿ Methods

| Category | Method | à¦•à¦¿ à¦•à¦°à§‡? |
|----------|--------|---------|
| **Filter** | Mutual Information | Feature-target dependency |
| | Chi-Squared | Statistical association |
| | ANOVA F-test | Class variance ratio |
| | CFS (Correlation) | Target correlation - redundancy |
| **Wrapper** | RFE (Logistic) | Iterative elimination |
| | RFE (Random Forest) | Tree-based elimination |
| **Embedded** | Lasso (L1) | Coefficient shrinkage |
| | Random Forest | Gini importance |
| | Gradient Boosting | Boosting importance |

## 2.3 à¦†à¦®à¦¾à¦¦à§‡à¦° Configuration

```python
K_FEATURES = 25  # à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ method 25à¦Ÿà¦¿ feature select à¦•à¦°à¦¬à§‡

# Data splits
X_train: (312, 51)  # Training set
X_val: (44, 51)     # Validation set
X_test: (42, 51)    # Test set
```

---

# 3. FILTER METHODS

Filter methods model à¦›à¦¾à¦¡à¦¼à¦¾à¦‡ statistical measures à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡ features score à¦•à¦°à§‡à¥¤ à¦à¦°à¦¾ fast à¦à¦¬à¦‚ model-agnostic (à¦¯à§‡à¦•à§‹à¦¨à§‹ classifier à¦à¦° à¦¸à¦¾à¦¥à§‡ à¦•à¦¾à¦œ à¦•à¦°à§‡)à¥¤

---

## 3.1 MUTUAL INFORMATION (MI)

### 3.1.1 Concept (à¦¸à¦¹à¦œ à¦­à¦¾à¦·à¦¾à¦¯à¦¼)

**Mutual Information** measure à¦•à¦°à§‡ à¦¦à§à¦‡à¦Ÿà¦¾ variable à¦à¦° à¦®à¦§à§à¦¯à§‡ à¦•à¦¤à¦Ÿà¦¾ "shared information" à¦†à¦›à§‡à¥¤

**Analogy:** à¦§à¦°à§à¦¨ à¦†à¦ªà¦¨à¦¿ à¦à¦•à¦œà¦¨ à¦®à¦¾à¦¨à§à¦·à¦•à§‡ à¦šà§‡à¦¨à§‡à¦¨ à¦¨à¦¾à¥¤ à¦¶à§à¦§à§ à¦¤à¦¾à¦° height à¦œà¦¾à¦¨à¦²à§‡ à¦•à¦¿ à¦¤à¦¾à¦° weight guess à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à¦¬à§‡à¦¨? à¦ªà¦¾à¦°à¦¬à§‡à¦¨ à¦•à¦¿à¦›à§à¦Ÿà¦¾! à¦•à¦¾à¦°à¦£ height à¦†à¦° weight à¦à¦° à¦®à¦§à§à¦¯à§‡ "mutual information" à¦†à¦›à§‡à¥¤

```
MI(X; Y) = "X à¦œà¦¾à¦¨à¦²à§‡ Y à¦¸à¦®à§à¦ªà¦°à§à¦•à§‡ à¦•à¦¤à¦Ÿà¦¾ à¦œà¦¾à¦¨à¦¾ à¦¯à¦¾à¦¯à¦¼"

High MI: Feature à¦œà¦¾à¦¨à¦²à§‡ class predict à¦•à¦°à¦¾ à¦¸à¦¹à¦œ
Low MI: Feature class à¦à¦° à¦¸à¦¾à¦¥à§‡ unrelated
```

### 3.1.2 Mathematical Definition

$$I(X; Y) = H(Y) - H(Y|X)$$

Where:
- $H(Y)$ = Entropy of Y (target class à¦à¦° uncertainty)
- $H(Y|X)$ = Conditional entropy (feature à¦œà¦¾à¦¨à¦²à§‡ target à¦à¦° uncertainty)

**Entropy:**
$$H(Y) = -\sum_{y} P(y) \log P(y)$$

### 3.1.3 à¦†à¦®à¦¾à¦¦à§‡à¦° Implementation

```python
from sklearn.feature_selection import SelectKBest, mutual_info_classif

# SelectKBest à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡à¦›à¦¿
mi_selector = SelectKBest(
    score_func=mutual_info_classif,  # Scikit-learn à¦à¦° MI function
    k=K_FEATURES                      # Top 25 features select à¦•à¦°à¦¬à§‡
)
mi_selector.fit(X_train_orig, y_train)

# Results extract à¦•à¦°à¦¾
mi_scores = mi_selector.scores_           # à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ feature à¦à¦° MI score
mi_selected_mask = mi_selector.get_support()  # Boolean mask: selected/not
mi_selected_features = [feature_cols[i] for i in range(len(feature_cols)) 
                        if mi_selected_mask[i]]
```

### 3.1.4 à¦•à§‡à¦¨ MI à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦²à¦¾à¦®?

| Advantage | Explanation |
|-----------|-------------|
| **Non-linear relationships à¦§à¦°à¦¤à§‡ à¦ªà¦¾à¦°à§‡** | Correlation à¦¶à§à¦§à§ linear à¦¦à§‡à¦–à§‡, MI nonlinear à¦“ à¦¦à§‡à¦–à§‡ |
| **No assumptions** | Distribution assume à¦•à¦°à§‡ à¦¨à¦¾ |
| **Scale-invariant** | Feature scaling à¦ result change à¦¹à¦¯à¦¼ à¦¨à¦¾ |

### 3.1.5 Limitations

- Estimation à¦ à¦•à¦¿à¦›à§ randomness à¦¥à¦¾à¦•à§‡
- Small samples à¦ inaccurate à¦¹à¦¤à§‡ à¦ªà¦¾à¦°à§‡
- Computationally expensive (large data à¦¤à§‡)

---

## 3.2 CHI-SQUARED (Ï‡Â²) TEST

### 3.2.1 Concept (à¦¸à¦¹à¦œ à¦­à¦¾à¦·à¦¾à¦¯à¦¼)

**Chi-Squared test** à¦¦à§‡à¦–à§‡ observed frequencies à¦†à¦° expected frequencies à¦à¦° à¦®à¦§à§à¦¯à§‡ à¦•à¦¤à¦Ÿà¦¾ differenceà¥¤

**Analogy:** à¦§à¦°à§à¦¨ à¦†à¦ªà¦¨à¦¿ dice roll à¦•à¦°à¦›à§‡à¦¨à¥¤ Fair dice à¦¹à¦²à§‡ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ number à¦¸à¦®à¦¾à¦¨ à¦¸à¦®à§à¦­à¦¾à¦¬à¦¨à¦¾à¦¯à¦¼ à¦†à¦¸à¦¾ à¦‰à¦šà¦¿à¦¤à¥¤ à¦•à¦¿à¦¨à§à¦¤à§ à¦¯à¦¦à¦¿ 6 à¦¬à§‡à¦¶à¦¿ à¦†à¦¸à§‡, à¦¤à¦¾à¦¹à¦²à§‡ dice "biased"à¥¤ Chi-squared à¦à¦‡ "bias" measure à¦•à¦°à§‡à¥¤

```
High Ï‡Â²: Feature class à¦à¦° à¦¸à¦¾à¦¥à§‡ strongly associated
Low Ï‡Â²: Feature à¦†à¦° class independent
```

### 3.2.2 Mathematical Definition

$$\chi^2 = \sum \frac{(O - E)^2}{E}$$

Where:
- $O$ = Observed frequency
- $E$ = Expected frequency (if independent)

### 3.2.3 à¦†à¦®à¦¾à¦¦à§‡à¦° Implementation

```python
from sklearn.feature_selection import chi2
from sklearn.preprocessing import MinMaxScaler

# âš ï¸ CRITICAL: Chi-squared requires NON-NEGATIVE values!
# à¦†à¦®à¦¾à¦¦à§‡à¦° features negative à¦¹à¦¤à§‡ à¦ªà¦¾à¦°à§‡ (e.g., MFCCs)
# à¦¤à¦¾à¦‡ MinMaxScaler à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡ [0, 1] range à¦ à¦†à¦¨à¦²à¦¾à¦®

scaler_minmax = MinMaxScaler()
X_train_positive = scaler_minmax.fit_transform(X_train_orig)

chi2_selector = SelectKBest(
    score_func=chi2,    # Chi-squared function
    k=K_FEATURES        # Top 25
)
chi2_selector.fit(X_train_positive, y_train)

chi2_scores = chi2_selector.scores_
chi2_selected_mask = chi2_selector.get_support()
chi2_selected_features = [feature_cols[i] for i in range(len(feature_cols)) 
                          if chi2_selected_mask[i]]
```

### 3.2.4 à¦•à§‡à¦¨ MinMaxScaler à¦¦à¦°à¦•à¦¾à¦°?

```python
# Problem:
MFCC_1_mean = -50.23  # Negative value!
chi2() â†’ ERROR! Ï‡Â² formula à¦¤à§‡ (O-E)Â²/E, E negative à¦¹à¦²à§‡ problem

# Solution:
MinMaxScaler: X_scaled = (X - X_min) / (X_max - X_min)
Result: à¦¸à¦¬ values 0 à¦¥à§‡à¦•à§‡ 1 à¦à¦° à¦®à¦§à§à¦¯à§‡
```

### 3.2.5 à¦•à§‡à¦¨ Chi-Squared à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦²à¦¾à¦®?

| Advantage | Explanation |
|-----------|-------------|
| **Simple & Fast** | Direct calculation, no iteration |
| **Well-established** | Statistical test since 1900 |
| **Works with discrete** | Originally for categorical data |

### 3.2.6 Limitations

- Requires non-negative values
- Assumes features are positive counts (à¦†à¦®à¦°à¦¾ scaling à¦•à¦°à§‡ à¦¸à¦®à¦¸à§à¦¯à¦¾ solve à¦•à¦°à¦²à¦¾à¦®)
- Only captures linear dependencies

---

## 3.3 ANOVA F-STATISTIC

### 3.3.1 Concept (à¦¸à¦¹à¦œ à¦­à¦¾à¦·à¦¾à¦¯à¦¼)

**ANOVA (Analysis of Variance)** à¦¦à§‡à¦–à§‡ different groups à¦à¦° means à¦•à¦¤à¦Ÿà¦¾ differentà¥¤

**Analogy:** à¦§à¦°à§à¦¨ à¦¦à§à¦‡à¦Ÿà¦¾ school à¦à¦° exam results à¦¦à§‡à¦–à¦›à§‡à¦¨à¥¤ School A à¦¤à§‡ average 80, School B à¦¤à§‡ average 60à¥¤ à¦•à¦¿à¦¨à§à¦¤à§ à¦ªà§à¦°à¦¶à§à¦¨ à¦¹à¦²à§‹ - à¦à¦‡ difference à¦•à¦¿ significant à¦¨à¦¾ just random variation?

```
                    Variance BETWEEN classes
F-statistic = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    Variance WITHIN classes

High F: Classes are well separated by this feature
Low F: Feature doesn't discriminate between classes
```

### 3.3.2 Mathematical Definition

$$F = \frac{\text{Between-group variance}}{\text{Within-group variance}} = \frac{MS_{between}}{MS_{within}}$$

Where:
$$MS_{between} = \frac{\sum n_i(\bar{x_i} - \bar{x})^2}{k-1}$$
$$MS_{within} = \frac{\sum\sum (x_{ij} - \bar{x_i})^2}{N-k}$$

### 3.3.3 à¦†à¦®à¦¾à¦¦à§‡à¦° Implementation

```python
from sklearn.feature_selection import f_classif

anova_selector = SelectKBest(
    score_func=f_classif,  # ANOVA F-test
    k=K_FEATURES           # Top 25
)
anova_selector.fit(X_train_orig, y_train)

anova_scores = anova_selector.scores_
anova_selected_mask = anova_selector.get_support()
anova_selected_features = [feature_cols[i] for i in range(len(feature_cols)) 
                           if anova_selected_mask[i]]
```

### 3.3.4 Visual Example

```
Feature: MFCC_4_std

           HC (Healthy)        PD (Parkinson's)
           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Values:    [2.1, 2.3, 2.0]     [4.5, 4.8, 4.2]
Mean:      2.13                 4.5
           
Between-class variance: HIGH (means are different: 2.13 vs 4.5)
Within-class variance: LOW (values close to their means)

F-statistic: HIGH â†’ Good feature! âœ“
```

### 3.3.5 à¦•à§‡à¦¨ ANOVA à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦²à¦¾à¦®?

| Advantage | Explanation |
|-----------|-------------|
| **Direct class separation** | Explicitly measures how well feature separates classes |
| **No model needed** | Pure statistical test |
| **Handles multiple classes** | Works for more than 2 classes too |

### 3.3.6 Limitations

- Assumes normal distribution
- Sensitive to outliers
- Only captures linear relationships

---

## 3.4 CORRELATION-BASED FEATURE SELECTION (CFS)

### 3.4.1 Concept (à¦¸à¦¹à¦œ à¦­à¦¾à¦·à¦¾à¦¯à¦¼)

CFS à¦†à¦²à¦¾à¦¦à¦¾ à¦•à¦¾à¦°à¦£ à¦à¦Ÿà¦¾ à¦¶à§à¦§à§ target correlation à¦¨à¦¾, **feature redundancy** à¦“ consider à¦•à¦°à§‡à¥¤

**Analogy:** à¦§à¦°à§à¦¨ à¦†à¦ªà¦¨à¦¿ cricket team à¦¬à¦¾à¦¨à¦¾à¦šà§à¦›à§‡à¦¨à¥¤ à¦†à¦ªà¦¨à¦¿ 5 à¦œà¦¨ batsman à¦šà¦¾à¦‡à¦²à§‡à¦¨à¥¤ à¦•à¦¿à¦¨à§à¦¤à§ à¦¯à¦¦à¦¿ 5 à¦œà¦¨à¦‡ opening batsman à¦¹à¦¯à¦¼, à¦¤à¦¾à¦¹à¦²à§‡ team unbalancedà¥¤ à¦†à¦ªà¦¨à¦¿ à¦šà¦¾à¦‡à¦¬à§‡à¦¨ different skills - opener, middle order, finisherà¥¤

```
CFS Strategy:
1. High correlation with target âœ“
2. Low correlation with other selected features âœ“

"Good features are highly correlated with target but 
 uncorrelated with each other"
```

### 3.4.2 CFS Merit Formula

$$Merit = \frac{k \cdot \overline{r_{cf}}}{\sqrt{k + k(k-1) \cdot \overline{r_{ff}}}}$$

Where:
- $k$ = number of selected features
- $\overline{r_{cf}}$ = average feature-class correlation
- $\overline{r_{ff}}$ = average feature-feature correlation

### 3.4.3 à¦†à¦®à¦¾à¦¦à§‡à¦° Implementation (Custom Function)

```python
def correlation_based_selection(X, y, feature_names, k=25):
    """
    Select features with:
    - High correlation with target
    - Low inter-feature correlation (redundancy)
    """
    
    # Step 1: Calculate feature-target correlation
    # à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ feature target à¦à¦° à¦¸à¦¾à¦¥à§‡ à¦•à¦¤à¦Ÿà¦¾ correlated
    feature_target_corr = np.array([
        np.abs(np.corrcoef(X[:, i], y)[0, 1]) 
        for i in range(X.shape[1])
    ])
    feature_target_corr = np.nan_to_num(feature_target_corr, 0)
    
    # Step 2: Calculate inter-feature correlation matrix
    # Features à¦¨à¦¿à¦œà§‡à¦¦à§‡à¦° à¦®à¦§à§à¦¯à§‡ à¦•à¦¤à¦Ÿà¦¾ correlated
    feature_corr_matrix = np.abs(np.corrcoef(X.T))
    np.fill_diagonal(feature_corr_matrix, 0)  # Diagonal = self-correlation = 0
    
    # Step 3: Greedy forward selection
    selected_indices = []
    remaining = list(range(X.shape[1]))
    
    for _ in range(k):  # k features select à¦•à¦°à¦¬à§‹
        best_score = -np.inf
        best_idx = None
        
        for idx in remaining:
            if len(selected_indices) == 0:
                # First feature: just use target correlation
                score = feature_target_corr[idx]
            else:
                # CFS merit formula
                k_selected = len(selected_indices) + 1
                
                # Sum of target correlations
                sum_target_corr = feature_target_corr[idx] + \
                                  sum(feature_target_corr[i] for i in selected_indices)
                
                # Sum of inter-feature correlations (redundancy penalty)
                sum_feature_corr = sum(feature_corr_matrix[idx, i] 
                                       for i in selected_indices)
                
                # Merit formula
                avg_target = sum_target_corr / k_selected
                avg_feature = sum_feature_corr / max(1, len(selected_indices))
                
                denominator = np.sqrt(k_selected + k_selected * (k_selected - 1) * avg_feature)
                score = (k_selected * avg_target) / max(denominator, 1e-10)
            
            if score > best_score:
                best_score = score
                best_idx = idx
        
        # Add best feature to selected set
        if best_idx is not None:
            selected_indices.append(best_idx)
            remaining.remove(best_idx)
    
    selected_features = [feature_names[i] for i in selected_indices]
    scores = dict(zip(feature_names, feature_target_corr))
    
    return selected_features, scores

# Call the function
cfs_selected_features, cfs_scores = correlation_based_selection(
    X_train_orig, y_train, feature_cols, k=K_FEATURES
)
```

### 3.4.4 Step-by-Step Example

```
Iteration 1:
  - No features selected yet
  - Select feature with highest target correlation
  - Winner: MFCC_4_std (corr = 0.45)

Iteration 2:
  - Already have: MFCC_4_std
  - For each remaining feature, calculate merit
  - MFCC_5_std has:
    - High target corr: 0.42 âœ“
    - Low corr with MFCC_4_std: 0.15 âœ“
    - Merit = high
  - MFCC_4_mean has:
    - High target corr: 0.40
    - High corr with MFCC_4_std: 0.85 âœ— (redundant!)
    - Merit = low
  - Winner: MFCC_5_std

...continue for 25 iterations...
```

### 3.4.5 à¦•à§‡à¦¨ CFS à¦¸à¦¬à¦šà§‡à¦¯à¦¼à§‡ à¦­à¦¾à¦²à§‹ perform à¦•à¦°à¦²à§‹?

| Reason | Explanation |
|--------|-------------|
| **Handles redundancy** | à¦…à¦¨à§à¦¯ methods à¦¶à§à¦§à§ target correlation à¦¦à§‡à¦–à§‡, CFS redundancy à¦“ à¦¦à§‡à¦–à§‡ |
| **Diverse feature set** | Different information capture à¦•à¦°à§‡ |
| **No overfitting** | Fewer correlated features = less noise |

**à¦†à¦®à¦¾à¦¦à§‡à¦° Result:** CFS achieved **Test F1 = 0.783** (Best among all methods!)

---

# 4. WRAPPER METHODS

Wrapper methods à¦†à¦¸à¦² ML model à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡ features evaluate à¦•à¦°à§‡à¥¤ Accurate à¦•à¦¿à¦¨à§à¦¤à§ slowà¥¤

---

## 4.1 RFE WITH LOGISTIC REGRESSION

### 4.1.1 Concept (à¦¸à¦¹à¦œ à¦­à¦¾à¦·à¦¾à¦¯à¦¼)

**RFE (Recursive Feature Elimination)** à¦à¦•à¦Ÿà¦¾ backward elimination processà¥¤

**Analogy:** à¦§à¦°à§à¦¨ à¦†à¦ªà¦¨à¦¿ 51 à¦œà¦¨ candidate à¦¥à§‡à¦•à§‡ 25 à¦œà¦¨ select à¦•à¦°à¦¬à§‡à¦¨ interview à¦¤à§‡à¥¤ à¦†à¦ªà¦¨à¦¿ à¦•à¦¿ à¦•à¦°à¦¬à§‡à¦¨?

Option 1: à¦¸à¦¬à¦¾à¦° score à¦¦à§‡à¦–à§‡ top 25 à¦¨à§‡à¦¬à§‡à¦¨ (Filter method)
Option 2: à¦ªà§à¦°à¦¤à¦¿ round à¦ worst performer à¦¬à¦¾à¦¦ à¦¦à§‡à¦¬à§‡à¦¨ (RFE method)

```
RFE Process:
Round 1: 51 features â†’ Train model â†’ Remove worst feature â†’ 50 features
Round 2: 50 features â†’ Train model â†’ Remove worst feature â†’ 49 features
...
Round 26: 26 features â†’ Train model â†’ Remove worst feature â†’ 25 features âœ“
```

### 4.1.2 à¦•à§‡à¦¨ Logistic Regression?

Logistic Regression à¦ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ feature à¦à¦° à¦à¦•à¦Ÿà¦¾ **coefficient** à¦¥à¦¾à¦•à§‡à¥¤

$$P(y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n)}}$$

- High |coefficient| = Important feature
- Low |coefficient| = Unimportant feature

### 4.1.3 à¦†à¦®à¦¾à¦¦à§‡à¦° Implementation

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# Base estimator: Logistic Regression
base_estimator = LogisticRegression(
    max_iter=1000,           # Maximum iterations
    random_state=42,         # Reproducibility
    class_weight='balanced'  # Handle class imbalance (HC â‰  PD count)
)

# RFE selector
rfe_selector = RFE(
    estimator=base_estimator,
    n_features_to_select=K_FEATURES,  # Final 25 features
    step=1,                           # Remove 1 feature per iteration
    verbose=0
)
rfe_selector.fit(X_train_orig, y_train)

# Results
rfe_selected_mask = rfe_selector.support_    # Boolean mask
rfe_rankings = rfe_selector.ranking_         # Ranking (1 = selected)
rfe_selected_features = [feature_cols[i] for i in range(len(feature_cols)) 
                         if rfe_selected_mask[i]]

# Convert rankings to scores (lower rank = higher score)
rfe_scores = {feat: 1.0 / rank for feat, rank in zip(feature_cols, rfe_rankings)}
```

### 4.1.4 Ranking System

```
rfe_rankings array:
[1, 1, 1, 3, 5, 1, 2, ...]

Meaning:
- Rank 1 = Selected (in final 25)
- Rank 2 = 26th best (eliminated 2nd to last)
- Rank 27 = Worst (eliminated first)
```

### 4.1.5 Complexity Analysis

```
Total iterations = 51 - 25 = 26 rounds
Each round: Train Logistic Regression on training data

Time complexity: O(n_rounds Ã— training_time)
```

---

## 4.2 RFE WITH RANDOM FOREST

### 4.2.1 Concept

Same RFE process à¦•à¦¿à¦¨à§à¦¤à§ **Random Forest** à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡ feature importance calculate à¦•à¦°à¦¾ à¦¹à¦¯à¦¼à¥¤

### 4.2.2 à¦•à§‡à¦¨ Random Forest?

Logistic Regression à¦¶à§à¦§à§ linear relationships à¦¦à§‡à¦–à§‡à¥¤ à¦•à¦¿à¦¨à§à¦¤à§ Random Forest:
- Nonlinear relationships à¦§à¦°à¦¤à§‡ à¦ªà¦¾à¦°à§‡
- Feature interactions capture à¦•à¦°à§‡
- More robust to outliers

### 4.2.3 à¦†à¦®à¦¾à¦¦à§‡à¦° Implementation

```python
# Base estimator: Random Forest (lighter version for speed)
rf_estimator = RandomForestClassifier(
    n_estimators=50,    # 50 trees (less for speed)
    random_state=42,
    max_depth=5,        # Limit depth (prevent overfitting, faster)
    n_jobs=-1           # Use all CPU cores
)

rfe_rf_selector = RFE(
    estimator=rf_estimator,
    n_features_to_select=K_FEATURES,
    step=2,              # Remove 2 features per iteration (faster)
    verbose=0
)
rfe_rf_selector.fit(X_train_orig, y_train)

rfe_rf_selected_mask = rfe_rf_selector.support_
rfe_rf_rankings = rfe_rf_selector.ranking_
rfe_rf_selected_features = [feature_cols[i] for i in range(len(feature_cols)) 
                            if rfe_rf_selected_mask[i]]
```

### 4.2.4 Key Differences from RFE-Logistic

| Parameter | RFE-Logistic | RFE-RF |
|-----------|--------------|--------|
| `step` | 1 | 2 (faster) |
| `n_estimators` | N/A | 50 |
| `max_depth` | N/A | 5 |
| Feature importance | Coefficients | Gini importance |

---

# 5. EMBEDDED METHODS

Embedded methods feature selection à¦Ÿà¦¾ model training à¦à¦° à¦¸à¦¾à¦¥à§‡à¦‡ à¦•à¦°à§‡ - separate step à¦¨à¦¾à¥¤

---

## 5.1 LASSO (L1 REGULARIZATION)

### 5.1.1 Concept (à¦¸à¦¹à¦œ à¦­à¦¾à¦·à¦¾à¦¯à¦¼)

**Lasso** regression à¦ **L1 penalty** add à¦•à¦°à§‡ à¦¯à§‡à¦Ÿà¦¾ coefficients à¦•à§‡ zero à¦¬à¦¾à¦¨à¦¿à¦¯à¦¼à§‡ à¦¦à§‡à¦¯à¦¼à¥¤

**Analogy:** à¦§à¦°à§à¦¨ à¦†à¦ªà¦¨à¦¾à¦° budget limitedà¥¤ à¦†à¦ªà¦¨à¦¿ à¦–à¦°à¦š à¦•à¦®à¦¾à¦¤à§‡ à¦šà¦¾à¦¨à¥¤ Lasso à¦¬à¦²à§‡ - "à¦¯à§‡à¦¸à¦¬ à¦œà¦¿à¦¨à¦¿à¦¸ à¦¨à¦¾ à¦¹à¦²à§‡à¦“ à¦šà¦²à¦¬à§‡, à¦¸à§‡à¦—à§à¦²à§‹à¦¤à§‡ à¦à¦•à¦¦à¦® à¦–à¦°à¦š à¦•à¦°à§‹ à¦¨à¦¾ (coefficient = 0)à¥¤"

```
Regular Regression Loss:
Loss = Sum(errorsÂ²)

Lasso Loss:
Loss = Sum(errorsÂ²) + Î± Ã— Sum(|coefficients|)
                       â†‘
                   L1 Penalty
```

### 5.1.2 à¦•à§‡à¦¨ Lasso Feature Selection à¦•à¦°à§‡?

L1 penalty à¦à¦° geometry à¦° à¦•à¦¾à¦°à¦£à§‡ à¦•à¦¿à¦›à§ coefficients exactly zero à¦¹à¦¯à¦¼à§‡ à¦¯à¦¾à¦¯à¦¼:

```
                    L2 (Ridge)              L1 (Lasso)
                    
Shape:              Circle                  Diamond
                      â¬¤                       â—‡
                                              
Effect:             Coefficients            Coefficients can be
                    shrink but              exactly ZERO
                    never zero              (automatic selection!)
```

### 5.1.3 à¦†à¦®à¦¾à¦¦à§‡à¦° Implementation

```python
from sklearn.linear_model import LassoCV

# LassoCV automatically finds best alpha using cross-validation
lasso = LassoCV(
    cv=5,              # 5-fold cross-validation
    random_state=42,
    max_iter=5000      # More iterations for convergence
)
lasso.fit(X_train_orig, y_train)

# Get coefficients (non-zero = important)
lasso_coefs = np.abs(lasso.coef_)

# Select top K features by coefficient magnitude
lasso_ranking = np.argsort(lasso_coefs)[::-1]  # Descending order
lasso_selected_indices = lasso_ranking[:K_FEATURES]  # Top 25
lasso_selected_features = [feature_cols[i] for i in lasso_selected_indices]

# Count truly non-zero coefficients
n_nonzero = np.sum(lasso_coefs > 1e-10)

print(f"Optimal alpha: {lasso.alpha_:.6f}")
print(f"Non-zero coefficients: {n_nonzero}")
```

### 5.1.4 Alpha Parameter

```
Î± (alpha) controls regularization strength:

Î± = 0:      No penalty â†’ All coefficients free â†’ Overfitting
Î± = small:  Weak penalty â†’ Few zeros â†’ More features
Î± = large:  Strong penalty â†’ Many zeros â†’ Fewer features
Î± = âˆ:      All coefficients = 0

LassoCV finds optimal Î± automatically!
```

### 5.1.5 à¦†à¦®à¦¾à¦¦à§‡à¦° Result

```
Optimal alpha: 0.001234
Non-zero coefficients: 38 (out of 51)
Selected top 25 by |coefficient|
```

---

## 5.2 RANDOM FOREST FEATURE IMPORTANCE

### 5.2.1 Concept (à¦¸à¦¹à¦œ à¦­à¦¾à¦·à¦¾à¦¯à¦¼)

Random Forest 100 à¦Ÿà¦¾ decision trees à¦¤à§ˆà¦°à¦¿ à¦•à¦°à§‡à¥¤ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ tree à¦¤à§‡ à¦¯à§‡ feature à¦­à¦¾à¦²à§‹ split à¦•à¦°à§‡ (classes separate à¦•à¦°à§‡), à¦¸à§‡à¦Ÿà¦¾ importantà¥¤

**Analogy:** à¦†à¦ªà¦¨à¦¿ 100 à¦œà¦¨ judge à¦•à§‡ à¦¬à¦²à¦²à§‡à¦¨ PD vs HC distinguish à¦•à¦°à¦¤à§‡à¥¤ à¦¯à§‡ feature à¦•à§‡ à¦¸à¦¬à¦šà§‡à¦¯à¦¼à§‡ à¦¬à§‡à¦¶à¦¿ judges à¦—à§à¦°à§à¦¤à§à¦¬ à¦¦à¦¿à¦²à§‹, à¦¸à§‡à¦Ÿà¦¾à¦‡ importantà¥¤

### 5.2.2 Gini Importance

$$Importance(f) = \sum_{nodes \text{ using } f} \frac{n_{node}}{n_{total}} \times \Delta Gini$$

Where:
- $\Delta Gini$ = Impurity reduction from the split
- $n_{node}$ = Samples reaching that node

### 5.2.3 à¦†à¦®à¦¾à¦¦à§‡à¦° Implementation

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel

rf_model = RandomForestClassifier(
    n_estimators=100,        # 100 trees
    random_state=42,
    max_depth=10,            # Limit depth
    class_weight='balanced', # Handle imbalance
    n_jobs=-1                # Parallel
)
rf_model.fit(X_train_orig, y_train)

# Get feature importances
rf_importances = rf_model.feature_importances_

# Select top K features using SelectFromModel
rf_selector = SelectFromModel(
    rf_model, 
    prefit=True,             # Already trained
    threshold=-np.inf,       # Don't threshold, just use max_features
    max_features=K_FEATURES  # Top 25
)
rf_selected_mask = rf_selector.get_support()
rf_selected_features = [feature_cols[i] for i in range(len(feature_cols)) 
                        if rf_selected_mask[i]]
```

### 5.2.4 Random Forest vs Single Decision Tree

```
Single Tree:        Unstable, varies with data
Random Forest:      Averages over 100 trees â†’ Stable importance scores
```

---

## 5.3 GRADIENT BOOSTING FEATURE IMPORTANCE

### 5.3.1 Concept (à¦¸à¦¹à¦œ à¦­à¦¾à¦·à¦¾à¦¯à¦¼)

**Gradient Boosting** sequentially trees build à¦•à¦°à§‡, à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ tree à¦†à¦—à§‡à¦° tree à¦à¦° mistakes correct à¦•à¦°à§‡à¥¤

**Analogy:** 
- Random Forest: 100 judges independently vote à¦•à¦°à§‡
- Gradient Boosting: Judge 2 tries to correct Judge 1's mistakes, Judge 3 corrects Judge 2's mistakes...

### 5.3.2 Difference from Random Forest

| Aspect | Random Forest | Gradient Boosting |
|--------|---------------|-------------------|
| Trees | Independent (parallel) | Sequential |
| Mistakes | Averaged out | Corrected iteratively |
| Speed | Faster | Slower |
| Overfitting | Less prone | More prone |

### 5.3.3 à¦†à¦®à¦¾à¦¦à§‡à¦° Implementation

```python
from sklearn.ensemble import GradientBoostingClassifier

gb_model = GradientBoostingClassifier(
    n_estimators=100,    # 100 boosting rounds
    learning_rate=0.1,   # Step size
    max_depth=5,         # Shallow trees (boosting à¦¤à§‡ shallow better)
    random_state=42
)
gb_model.fit(X_train_orig, y_train)

gb_importances = gb_model.feature_importances_

# Select top K features
gb_ranking_idx = np.argsort(gb_importances)[::-1][:K_FEATURES]
gb_selected_features = [feature_cols[i] for i in gb_ranking_idx]
```

---

# 6. CONSENSUS STRATEGY

## 6.1 à¦•à§‡à¦¨ Consensus?

à¦†à¦®à¦°à¦¾ 9à¦Ÿà¦¾ different methods run à¦•à¦°à¦²à¦¾à¦®à¥¤ à¦à¦–à¦¨ à¦•à§‹à¦¨à¦Ÿà¦¾ à¦¬à¦¿à¦¶à§à¦¬à¦¾à¦¸ à¦•à¦°à¦¬à§‹?

**Problem:**
- à¦•à§‹à¦¨à§‹ method perfect à¦¨à¦¾
- Different methods different assumptions à¦•à¦°à§‡
- à¦•à¦¿à¦›à§ methods certain features à¦•à§‡ overvalue à¦•à¦°à§‡

**Solution:** **Ensemble/Voting approach!**

```
à¦¯à¦¦à¦¿ à¦à¦•à¦Ÿà¦¾ feature 5+ methods à¦¦à§à¦¬à¦¾à¦°à¦¾ selected à¦¹à¦¯à¦¼,
à¦¸à§‡à¦Ÿà¦¾ probably genuinely importantà¥¤

"Wisdom of the crowd"
```

## 6.2 à¦†à¦®à¦¾à¦¦à§‡à¦° Implementation

```python
# Count how many methods selected each feature
all_selected = {}
for method, results in feature_selection_results.items():
    for feat in results['selected_features']:
        if feat not in all_selected:
            all_selected[feat] = []
        all_selected[feat].append(method)

feature_counts = {feat: len(methods) for feat, methods in all_selected.items()}

# Strategy: Select features chosen by at least 5 out of 9 methods
MIN_METHODS = 5
consensus_features = [f for f, c in feature_counts.items() if c >= MIN_METHODS]

print(f"Consensus features: {len(consensus_features)}")
# Result: 24 features
```

## 6.3 à¦•à§‡à¦¨ Threshold = 5?

```
9 methods:
- 5/9 = 55.6% (majority vote)
- This is the democratic threshold

Too low (e.g., 3): Too many features, includes unreliable ones
Too high (e.g., 8): Too few features, may miss good ones
```

## 6.4 Consensus Results

```
Perfect Agreement (9/9 methods): 4 features
  - MFCC_4_std
  - MFCC_5_std
  - MFCC_11_mean
  - MFCC_13_mean

Strong Agreement (8/9): 3 features
  - Shimmer_APQ3
  - Shimmer_DDA
  - MFCC_2_mean

Good Agreement (7/9): 7 features
  - MDVP_Jitter_percent, MDVP_Jitter_Abs, MDVP_PPQ
  - RPDE
  - MFCC_5_mean, MFCC_12_mean
  - (and more)

Total Consensus (5+/9): 24 features
```

---

# 7. FINAL RESULTS & ANALYSIS

## 7.1 Performance Comparison

| Method | Type | Test F1 |
|--------|------|---------|
| **CFS (Correlation)** | Filter | **0.783** ğŸ¥‡ |
| Chi-Squared | Filter | 0.773 ğŸ¥ˆ |
| ANOVA F-test | Filter | 0.773 ğŸ¥ˆ |
| RFE (Random Forest) | Wrapper | 0.766 |
| RFE (Logistic) | Wrapper | 0.762 |
| **CONSENSUS** | Hybrid | 0.756 |
| Random Forest | Embedded | 0.750 |
| Gradient Boosting | Embedded | 0.743 |
| Mutual Information | Filter | 0.727 |
| Lasso (L1) | Embedded | 0.714 |

## 7.2 Key Findings

### Finding 1: Filter Methods Won!
```
Top 3 all Filter methods!
Why? Our dataset is small (398 samples)
- Filter methods are more stable
- Wrapper/Embedded can overfit
```

### Finding 2: CFS is Best
```
CFS considers BOTH:
1. Feature-target correlation âœ“
2. Feature-feature correlation (redundancy) âœ“

Other methods only consider #1
```

### Finding 3: Consensus is Robust
```
Consensus F1 = 0.756 (4th best)
But more reliable because:
- Multiple methods agree
- Less likely to overfit
- More generalizable
```

## 7.3 Final Feature Set

**51 features â†’ 24 features (53% reduction)**

```
Category-wise:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Jitter:      5/5 = 100% selected ğŸ†             â”‚
â”‚ MFCCs:       12/26 = 46% selected               â”‚
â”‚ Shimmer:     3/6 = 50% selected                 â”‚
â”‚ Harmonic:    1/2 = 50% selected                 â”‚
â”‚ Time-Domain: 1/2 = 50% selected                 â”‚
â”‚ Pitch:       1/3 = 33% selected                 â”‚
â”‚ Nonlinear:   1/6 = 17% selected                 â”‚
â”‚ Spectral:    0/1 = 0% selected                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 7.4 Conclusion

à¦†à¦®à¦¾à¦¦à§‡à¦° multi-method feature selection approach:

1. âœ… **Robust:** 9 different perspectives à¦¥à§‡à¦•à§‡ features evaluate
2. âœ… **Interpretable:** à¦¯à§‡ features à¦¸à¦¬à¦¾à¦‡ important à¦¬à¦²à§‡ à¦¸à§‡à¦—à§à¦²à§‹à¦‡ select
3. âœ… **Efficient:** 53% dimensionality reduction
4. âœ… **Clinical relevance:** Jitter (voice shakiness) à¦¸à¦¬à¦šà§‡à¦¯à¦¼à§‡ important - à¦à¦Ÿà¦¾ PD à¦° known symptom!

---

# ğŸ“š REFERENCES

1. Scikit-learn Feature Selection: https://scikit-learn.org/stable/modules/feature_selection.html
2. Hall, M. A. (1999). Correlation-based Feature Selection for Machine Learning. PhD Thesis.
3. Guyon, I., & Elisseeff, A. (2003). An introduction to variable and feature selection. JMLR.
4. Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. JRSS.

---

*Document created for Parkinson's Disease Detection Project*
*Feature Selection Methodology Documentation*
*Date: December 2024*
